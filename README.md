# Отчет по практической работе: Полная реализация ETL-процесса с визуализацией в DataLens

### Задание 1: Работа с Yandex DataTransfer

На этом этапе была подготовлена исходная база данных в Yandex Database и настроен трансфер данных в Object Storage.

**Действия:**
1.  **Создана таблица в YDB**: С помощью YQL-скрипта `task1/create_ydb_table.sql` была создана таблица `transactions_v2` для исходных данных.
2.  **Настроен трансфер**: В сервисе Yandex Data Transfer был создан трансфер типа **Источник (Yandex Database) -> Приемник (Object Storage)**. Это позволило выгрузить данные из таблицы YDB в бакет S3 в формате CSV, который стал источником для следующего этапа.
3. **Созданы скриншоты работы**

```sql
-- ЗАДАНИЕ 1/create_ydb_table.sql
CREATE TABLE transactions_v2 (
    msno Utf8,
    payment_method_id Int32,
    payment_plan_days Int32,
    plan_list_price Int32,
    actual_amount_paid Int32,
    is_auto_renew Int8,
    transaction_date Utf8,
    membership_expire_date Utf8,
    is_cancel Int8,
    PRIMARY KEY (msno)
); 
```

### Задание 2: Автоматизация работы с Yandex Data Processing

Для автоматизации процесса очистки данных был разработан DAG в Apache Airflow.

**Действия:**
1.  **Создан DAG `DATA_INGEST_V2`** (`task2/dags/data_processing_dag.py`), который полностью автоматизирует пакетную обработку.
2.  **Логика DAG**:
    *   **Создание временного кластера Data Proc**: DAG динамически создает кластер Spark только на время выполнения задачи.
    *   **Запуск PySpark-задания**: На кластере запускается скрипт `task2/scripts/clean_data.py`, который читает CSV-файл из S3, очищает его, преобразует типы данных и сохраняет результат в формате **Parquet**.
    *   **Удаление кластера**: После завершения задачи кластер автоматически удаляется для экономии ресурсов.
3. **Созданы скриншоты работы**

### Задание 3: Работа с топиками Apache Kafka®

Для реализации потоковой аналитики был создан второй DAG, который запускает два Spark-задания на **постоянно работающем** кластере Data Proc.

**Действия:**
1.  **Подготовлена целевая БД**: Скриптом `task1/create_postgres_table.sql` создана таблица в PostgreSQL для хранения конечных данных.
2.  **Создан DAG `STREAMING_JOBS`** (`task3/dags/streaming_dag.py`), который запускает продюсера и консьюмера.
3.  **`kafka_producer.py`**: Читает очищенный Parquet-файл и в бесконечном цикле отправляет записи в топик Kafka, имитируя живой поток данных.
4.  **`kafka_consumer_to_postgres.py`**: Читает данные из Kafka и загружает их в PostgreSQL, используя логику **UPSERT** (`INSERT ... ON CONFLICT UPDATE`).
5. **Созданы скриншоты работы**

### Задание 4: Визуализация в DataLens

<img width="1512" alt="Screenshot 2025-06-19 at 21 52 13" src="https://github.com/user-attachments/assets/a1a22b08-4449-4687-a066-3ea5d0fd67b1" />

## Заключение

В ходе выполнения работы был успешно построен и автоматизирован комплексный ETL-конвейер на базе сервисов Yandex Cloud, полностью соответствующий техническому заданию. Реализованное решение демонстрирует способность обрабатывать как большие объемы исторических данных, так и непрерывные потоки данных, подготавливая их для дальнейшего анализа и визуализации. 
